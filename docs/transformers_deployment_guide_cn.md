# ğŸš€ MiniMax æ¨¡å‹ Transformers éƒ¨ç½²æŒ‡å—

## ğŸ“– ç®€ä»‹

æœ¬æŒ‡å—å°†å¸®åŠ©æ‚¨ä½¿ç”¨ [Transformers](https://huggingface.co/docs/transformers/index) åº“éƒ¨ç½² MiniMax-M1 æ¨¡å‹ã€‚Transformers æ˜¯ä¸€ä¸ªå¹¿æ³›ä½¿ç”¨çš„æ·±åº¦å­¦ä¹ åº“ï¼Œæä¾›äº†ä¸°å¯Œçš„é¢„è®­ç»ƒæ¨¡å‹å’Œçµæ´»çš„æ¨¡å‹æ“ä½œæ¥å£ã€‚

## ğŸ› ï¸ ç¯å¢ƒå‡†å¤‡

### å®‰è£… Transformers

```bash
pip install transformers torch accelerate
```

## ğŸ“‹ åŸºæœ¬ä½¿ç”¨ç¤ºä¾‹

é¢„è®­ç»ƒæ¨¡å‹å¯ä»¥æŒ‰ç…§ä»¥ä¸‹æ–¹å¼ä½¿ç”¨ï¼š

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig

MODEL_PATH = "{MODEL_PATH}"
model = AutoModelForCausalLM.from_pretrained(MODEL_PATH, device_map="auto", trust_remote_code=True)
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)

messages = [
    {"role": "user", "content": [{"type": "text", "text": "What is your favourite condiment?"}]},
    {"role": "assistant", "content": [{"type": "text", "text": "Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!"}]},
    {"role": "user", "content": [{"type": "text", "text": "Do you have mayonnaise recipes?"}]}
]

text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)

model_inputs = tokenizer(text, return_tensors="pt").to(model.device)

generation_config = GenerationConfig(
    max_new_tokens=20,
    eos_token_id=tokenizer.eos_token_id,
    use_cache=True,
)

generated_ids = model.generate(**model_inputs, generation_config=generation_config)

generated_ids = [
    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
]

response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
print(response)
```

## âš¡ æ€§èƒ½ä¼˜åŒ–

### ä½¿ç”¨ Flash Attention åŠ é€Ÿ

ä¸Šé¢çš„ä»£ç ç‰‡æ®µå±•ç¤ºäº†ä¸ä½¿ç”¨ä»»ä½•ä¼˜åŒ–æŠ€å·§çš„æ¨ç†è¿‡ç¨‹ã€‚ä½†é€šè¿‡åˆ©ç”¨ [Flash Attention](../perf_train_gpu_one#flash-attention-2)ï¼Œå¯ä»¥å¤§å¹…åŠ é€Ÿæ¨¡å‹ï¼Œå› ä¸ºå®ƒæä¾›äº†æ¨¡å‹å†…éƒ¨ä½¿ç”¨çš„æ³¨æ„åŠ›æœºåˆ¶çš„æ›´å¿«å®ç°ã€‚

é¦–å…ˆï¼Œç¡®ä¿å®‰è£…æœ€æ–°ç‰ˆæœ¬çš„ Flash Attention 2ï¼š

```bash
pip install -U flash-attn --no-build-isolation
```

è¿˜è¦ç¡®ä¿æ‚¨æ‹¥æœ‰ä¸ Flash-Attention 2 å…¼å®¹çš„ç¡¬ä»¶ã€‚åœ¨[Flash Attention å®˜æ–¹ä»“åº“](https://github.com/Dao-AILab/flash-attention)çš„å®˜æ–¹æ–‡æ¡£ä¸­äº†è§£æ›´å¤šä¿¡æ¯ã€‚æ­¤å¤–ï¼Œè¯·ç¡®ä¿ä»¥åŠç²¾åº¦ï¼ˆä¾‹å¦‚ `torch.float16`ï¼‰åŠ è½½æ¨¡å‹ã€‚

è¦ä½¿ç”¨ Flash Attention-2 åŠ è½½å’Œè¿è¡Œæ¨¡å‹ï¼Œè¯·å‚è€ƒä»¥ä¸‹ä»£ç ç‰‡æ®µï¼š

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

MODEL_PATH = "{MODEL_PATH}"
model = AutoModelForCausalLM.from_pretrained(MODEL_PATH, trust_remote_code=True, torch_dtype=torch.float16, attn_implementation="flash_attention_2", device_map="auto")
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)

prompt = "My favourite condiment is"

model_inputs = tokenizer([prompt], return_tensors="pt").to("cuda")
generated_ids = model.generate(**model_inputs, max_new_tokens=100, do_sample=True)
response = tokenizer.batch_decode(generated_ids)[0]
print(response)
```

## ğŸ“® è·å–æ”¯æŒ

å¦‚æœæ‚¨åœ¨éƒ¨ç½² MiniMax-M1 æ¨¡å‹è¿‡ç¨‹ä¸­é‡åˆ°ä»»ä½•é—®é¢˜ï¼š
- è¯·æŸ¥çœ‹æˆ‘ä»¬çš„å®˜æ–¹æ–‡æ¡£
- é€šè¿‡å®˜æ–¹æ¸ é“è”ç³»æˆ‘ä»¬çš„æŠ€æœ¯æ”¯æŒå›¢é˜Ÿ
- åœ¨æˆ‘ä»¬çš„ GitHub ä»“åº“æäº¤ Issue

æˆ‘ä»¬ä¼šæŒç»­ä¼˜åŒ– Transformers ä¸Šçš„éƒ¨ç½²ä½“éªŒï¼Œæ¬¢è¿æ‚¨çš„åé¦ˆï¼
