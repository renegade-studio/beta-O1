# üöÄ Guia de Deploy dos Modelos MiniMax com vLLM

[vLLM‰∏≠ÊñáÁâàÈÉ®ÁΩ≤ÊåáÂçó](./vllm_deployment_guide_cn.md)

## üìñ Introdu√ß√£o

Recomendamos utilizar o [vLLM](https://docs.vllm.ai/en/latest/) para fazer o deploy do modelo [MiniMax-M1](https://huggingface.co/MiniMaxAI/MiniMax-M1-40k). Com base nos nossos testes, o vLLM apresenta excelente desempenho ao executar este modelo, oferecendo as seguintes vantagens:

- üî• Desempenho excepcional em throughput de servi√ßo
- ‚ö° Gerenciamento de mem√≥ria eficiente e inteligente
- üì¶ Capacidade robusta de processamento de requisi√ß√µes em lote
- ‚öôÔ∏è Otimiza√ß√£o profunda de desempenho em baixo n√≠vel

O modelo MiniMax-M1 pode ser executado de forma eficiente em um servidor √∫nico equipado com 8 GPUs H800 ou 8 GPUs H20. Em termos de configura√ß√£o de hardware, um servidor com 8 GPUs H800 consegue processar entradas de contexto com at√© 2 milh√µes de tokens, enquanto um servidor equipado com 8 GPUs H20 suporta contextos ultra longos de at√© 5 milh√µes de tokens.

## üíæ Obtendo os Modelos MiniMax

### Download do Modelo MiniMax-M1

Voc√™ pode baixar o modelo diretamente do nosso reposit√≥rio oficial no HuggingFace: [MiniMax-M1-40k](https://huggingface.co/MiniMaxAI/MiniMax-M1-40k) ou [MiniMax-M1-80k](https://huggingface.co/MiniMaxAI/MiniMax-M1-80k).

Comando para download:
```
pip install -U huggingface-hub
huggingface-cli download MiniMaxAI/MiniMax-M1-40k

# huggingface-cli download MiniMaxAI/MiniMax-M1-80k

# Se voc√™ encontrar problemas de rede, pode configurar um proxy

export HF\_ENDPOINT=[https://hf-mirror.com](https://hf-mirror.com)
```

Ou fa√ßa o download usando git:

```bash
git lfs install
git clone https://huggingface.co/MiniMaxAI/MiniMax-M1-40k
git clone https://huggingface.co/MiniMaxAI/MiniMax-M1-80k
```

‚ö†Ô∏è **Aten√ß√£o Importante**: Certifique-se de que o [Git LFS](https://git-lfs.github.com/) est√° instalado no seu sistema, pois ele √© necess√°rio para baixar completamente os arquivos de pesos do modelo.

## üõ†Ô∏è Op√ß√µes de Deploy

### Op√ß√£o 1: Deploy Utilizando Docker (Recomendado)

Para garantir consist√™ncia e estabilidade no ambiente de deployment, recomendamos utilizar Docker.

‚ö†Ô∏è **Requisitos de Vers√£o**:

* O modelo MiniMax-M1 requer vLLM na vers√£o 0.9.2 ou superior para suporte completo.
* Nota especial: Si se utiliza una versi√≥n de vLLM inferior a 0.9.2, pueden surgir problemas de incompatibilidad o precisi√≥n incorrecta del modelo:

  * Para m√°s detalles, consulta: [Fix minimax model cache & lm_head precision #19592](https://github.com/vllm-project/vllm/pull/19592)

1. Obtenha a imagem do container:

```bash
docker pull vllm/vllm-openai:v0.8.3
```

2. Execute o container:

```bash
# Defina vari√°veis de ambiente
IMAGE=vllm/vllm-openai:v0.8.3
MODEL_DIR=<caminho onde est√£o os modelos>
CODE_DIR=<caminho onde est√° o c√≥digo>
NAME=MiniMaxImage

# Configura√ß√£o do Docker run
DOCKER_RUN_CMD="--network=host --privileged --ipc=host --ulimit memlock=-1 --shm-size=2gb --rm --gpus all --ulimit stack=67108864"

# Inicie o container
sudo docker run -it \
    -v $MODEL_DIR:$MODEL_DIR \
    -v $CODE_DIR:$CODE_DIR \
    --name $NAME \
    $DOCKER_RUN_CMD \
    $IMAGE /bin/bash

cd $CODE_DIR
git clone https://github.com/vllm-project/vllm.git
cd vllm
pip install -e .
```

üí° Se voc√™ estiver utilizando outra configura√ß√£o de ambiente, consulte o [Guia de Instala√ß√£o do vLLM](https://docs.vllm.ai/en/latest/getting_started/installation.html).

## üöÄ Inicializando o Servi√ßo

### Iniciando o Servi√ßo com MiniMax-M1

```bash
export SAFETENSORS_FAST_GPU=1
export VLLM_USE_V1=0
python3 -m vllm.entrypoints.openai.api_server \
--model <caminho onde est√£o os modelos> \
--tensor-parallel-size 8 \
--trust-remote-code \
--quantization experts_int8  \
--max_model_len 4096 \
--dtype bfloat16
```

### Exemplo de Chamada via API

```bash
curl http://localhost:8000/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "MiniMaxAI/MiniMax-M1",
        "messages": [
            {"role": "system", "content": [{"type": "text", "text": "You are a helpful assistant."}]},
            {"role": "user", "content": [{"type": "text", "text": "Who won the world series in 2020?"}]}
        ]
    }'
```

## ‚ùó Problemas Comuns

### Problemas ao Carregar M√≥dulos

Se voc√™ encontrar o erro:

```
import vllm._C  # noqa
ModuleNotFoundError: No module named 'vllm._C'
```

Ou

```
MiniMax-M1 model is not currently supported
```

Disponibilizamos duas solu√ß√µes:

#### Solu√ß√£o 1: Copiar Arquivos de Depend√™ncia

```bash
cd <diret√≥rio de trabalho>
git clone https://github.com/vllm-project/vllm.git
cd vllm
cp /usr/local/lib/python3.12/dist-packages/vllm/*.so vllm 
cp -r /usr/local/lib/python3.12/dist-packages/vllm/vllm_flash_attn/* vllm/vllm_flash_attn
```

#### Solu√ß√£o 2: Instalar a partir do C√≥digo-Fonte

```bash
cd <diret√≥rio de trabalho>
git clone https://github.com/vllm-project/vllm.git

cd vllm/
pip install -e .
```

## üìÆ Suporte

Se voc√™ tiver qualquer problema durante o deploy do modelo MiniMax-M1:

* Consulte nossa documenta√ß√£o oficial
* Entre em contato com nossa equipe de suporte t√©cnico pelos canais oficiais
* Abra uma [Issue](https://github.com/MiniMax-AI/MiniMax-M1/issues) no nosso reposit√≥rio do GitHub

Estamos constantemente otimizando a experi√™ncia de deploy deste modelo e valorizamos muito seu feedback!
