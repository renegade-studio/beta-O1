# ğŸš€ MiniMax æ¨¡å‹ vLLM éƒ¨ç½²æŒ‡å—

## ğŸ“– ç®€ä»‹

æˆ‘ä»¬æ¨èä½¿ç”¨ [vLLM](https://docs.vllm.ai/en/latest/) æ¥éƒ¨ç½² [MiniMax-M1](https://huggingface.co/MiniMaxAI/MiniMax-M1-40k) æ¨¡å‹ã€‚ç»è¿‡æˆ‘ä»¬çš„æµ‹è¯•ï¼ŒvLLM åœ¨éƒ¨ç½²è¿™ä¸ªæ¨¡å‹æ—¶è¡¨ç°å‡ºè‰²ï¼Œå…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š

- ğŸ”¥ å“è¶Šçš„æœåŠ¡ååé‡æ€§èƒ½
- âš¡ é«˜æ•ˆæ™ºèƒ½çš„å†…å­˜ç®¡ç†æœºåˆ¶
- ğŸ“¦ å¼ºå¤§çš„æ‰¹é‡è¯·æ±‚å¤„ç†èƒ½åŠ›
- âš™ï¸ æ·±åº¦ä¼˜åŒ–çš„åº•å±‚æ€§èƒ½

MiniMax-M1 æ¨¡å‹å¯åœ¨å•å°é…å¤‡8ä¸ªH800æˆ–8ä¸ªH20 GPUçš„æœåŠ¡å™¨ä¸Šé«˜æ•ˆè¿è¡Œã€‚åœ¨ç¡¬ä»¶é…ç½®æ–¹é¢ï¼Œæ­è½½8ä¸ªH800 GPUçš„æœåŠ¡å™¨å¯å¤„ç†é•¿è¾¾200ä¸‡tokençš„ä¸Šä¸‹æ–‡è¾“å…¥ï¼Œè€Œé…å¤‡8ä¸ªH20 GPUçš„æœåŠ¡å™¨åˆ™èƒ½å¤Ÿæ”¯æŒé«˜è¾¾500ä¸‡tokençš„è¶…é•¿ä¸Šä¸‹æ–‡å¤„ç†èƒ½åŠ›ã€‚

## ğŸ’¾ è·å– MiniMax æ¨¡å‹

### MiniMax-M1 æ¨¡å‹è·å–

æ‚¨å¯ä»¥ä»æˆ‘ä»¬çš„å®˜æ–¹ HuggingFace ä»“åº“ä¸‹è½½æ¨¡å‹ï¼š[MiniMax-M1-40k](https://huggingface.co/MiniMaxAI/MiniMax-M1-40k)ã€[MiniMax-M1-80k](https://huggingface.co/MiniMaxAI/MiniMax-M1-80k)

ä¸‹è½½å‘½ä»¤ï¼š
```
pip install -U huggingface-hub
huggingface-cli download MiniMaxAI/MiniMax-M1-40k
# huggingface-cli download MiniMaxAI/MiniMax-M1-80k

# å¦‚æœé‡åˆ°ç½‘ç»œé—®é¢˜ï¼Œå¯ä»¥è®¾ç½®ä»£ç†
export HF_ENDPOINT=https://hf-mirror.com
```

æˆ–è€…ä½¿ç”¨ git ä¸‹è½½ï¼š

```bash
git lfs install
git clone https://huggingface.co/MiniMaxAI/MiniMax-M1-40k
git clone https://huggingface.co/MiniMaxAI/MiniMax-M1-80k
```

âš ï¸ **é‡è¦æç¤º**ï¼šè¯·ç¡®ä¿ç³»ç»Ÿå·²å®‰è£… [Git LFS](https://git-lfs.github.com/)ï¼Œè¿™å¯¹äºå®Œæ•´ä¸‹è½½æ¨¡å‹æƒé‡æ–‡ä»¶æ˜¯å¿…éœ€çš„ã€‚

## ğŸ› ï¸ éƒ¨ç½²æ–¹æ¡ˆ

### æ–¹æ¡ˆï¼šä½¿ç”¨ Docker éƒ¨ç½²ï¼ˆæ¨èï¼‰

ä¸ºç¡®ä¿éƒ¨ç½²ç¯å¢ƒçš„ä¸€è‡´æ€§å’Œç¨³å®šæ€§ï¼Œæˆ‘ä»¬æ¨èä½¿ç”¨ Docker è¿›è¡Œéƒ¨ç½²ã€‚

âš ï¸ **ç‰ˆæœ¬è¦æ±‚**ï¼š
- åŸºç¡€è¦æ±‚ï¼švLLM ç‰ˆæœ¬å¿…é¡» â‰¥ 0.9.2ï¼Œä»¥ç¡®ä¿å¯¹ MiniMax-M1 æ¨¡å‹çš„å®Œæ•´æ”¯æŒ
- ç‰¹æ®Šè¯´æ˜ï¼šå¦‚æœä½¿ç”¨ä½äº 0.9.2 çš„ vLLM ç‰ˆæœ¬ï¼Œä¼šé‡è§æ— æ³•æ”¯æŒè¯¥æ¨¡å‹æˆ–è€…ç²¾åº¦ä¸æ­£ç¡®çš„æƒ…å†µï¼š
  - è¯¦æƒ…è§ï¼š[Fix minimax model cache & lm_head precision #19592](https://github.com/vllm-project/vllm/pull/19592)

1. è·å–å®¹å™¨é•œåƒï¼š

ç›®å‰ vLLM å®˜æ–¹è¿˜æœªæ¨å‡ºv0.9.2ç‰ˆæœ¬ dockerï¼Œæˆ‘ä»¬ä»¥ v0.8.3 ä¸ºä¾‹å­è¿›è¡Œæ‰‹åŠ¨ç¼–è¯‘ vLLMï¼š
```bash
docker pull vllm/vllm-openai:v0.8.3
```

2. è¿è¡Œå®¹å™¨ï¼š
```bash
# è®¾ç½®ç¯å¢ƒå˜é‡
IMAGE=vllm/vllm-openai:v0.8.3
MODEL_DIR=<æ¨¡å‹å­˜æ”¾è·¯å¾„>
CODE_DIR=<ä»£ç è·¯å¾„>
NAME=MiniMaxImage

# Dockerè¿è¡Œé…ç½®
DOCKER_RUN_CMD="--network=host --privileged --ipc=host --ulimit memlock=-1 --shm-size=2gb --rm --gpus all --ulimit stack=67108864"

# å¯åŠ¨å®¹å™¨
sudo docker run -it \
    -v $MODEL_DIR:$MODEL_DIR \
    -v $CODE_DIR:$CODE_DIR \
    --name $NAME \
    $DOCKER_RUN_CMD \
    $IMAGE /bin/bash

# ç¼–è¯‘ vLLM
cd $CODE_DIR
git clone https://github.com/vllm-project/vllm.git
cd vllm
pip install -e .
```

ğŸ’¡ å¦‚æœæ‚¨ä½¿ç”¨å…¶ä»–ç¯å¢ƒé…ç½®ï¼Œè¯·å‚è€ƒ [vLLM å®‰è£…æŒ‡å—](https://docs.vllm.ai/en/latest/getting_started/installation.html)

## ğŸš€ å¯åŠ¨æœåŠ¡

### å¯åŠ¨ MiniMax-M1 æœåŠ¡

```bash
export SAFETENSORS_FAST_GPU=1
export VLLM_USE_V1=0
python3 -m vllm.entrypoints.openai.api_server \
--model <æ¨¡å‹å­˜æ”¾è·¯å¾„> \
--tensor-parallel-size 8 \
--trust-remote-code \
--quantization experts_int8  \
--max_model_len 4096 \
--dtype bfloat16
```

### API è°ƒç”¨ç¤ºä¾‹

```bash
curl http://localhost:8000/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "MiniMaxAI/MiniMax-M1",
        "messages": [
            {"role": "system", "content": [{"type": "text", "text": "You are a helpful assistant."}]},
            {"role": "user", "content": [{"type": "text", "text": "Who won the world series in 2020?"}]}
        ]
    }'
```

## â— å¸¸è§é—®é¢˜

### æ¨¡å—åŠ è½½é—®é¢˜
å¦‚æœé‡åˆ°ä»¥ä¸‹é”™è¯¯ï¼š
```
import vllm._C  # noqa
ModuleNotFoundError: No module named 'vllm._C'
```

æˆ–

```
å½“å‰å¹¶ä¸æ”¯æŒ MiniMax-M1 æ¨¡å‹
```

æˆ‘ä»¬æä¾›ä¸¤ç§è§£å†³æ–¹æ¡ˆï¼š

#### è§£å†³æ–¹æ¡ˆä¸€ï¼šå¤åˆ¶ä¾èµ–æ–‡ä»¶
```bash
cd <å·¥ä½œç›®å½•>
git clone https://github.com/vllm-project/vllm.git
cd vllm
cp /usr/local/lib/python3.12/dist-packages/vllm/*.so vllm 
cp -r /usr/local/lib/python3.12/dist-packages/vllm/vllm_flash_attn/* vllm/vllm_flash_attn
```

#### è§£å†³æ–¹æ¡ˆäºŒï¼šä»æºç å®‰è£…
```bash
cd <å·¥ä½œç›®å½•>
git clone https://github.com/vllm-project/vllm.git

cd vllm/
pip install -e .
```

## ğŸ“® è·å–æ”¯æŒ

å¦‚æœæ‚¨åœ¨éƒ¨ç½² MiniMax-M1 æ¨¡å‹è¿‡ç¨‹ä¸­é‡åˆ°ä»»ä½•é—®é¢˜ï¼š
- è¯·æŸ¥çœ‹æˆ‘ä»¬çš„å®˜æ–¹æ–‡æ¡£
- é€šè¿‡å®˜æ–¹æ¸ é“è”ç³»æˆ‘ä»¬çš„æŠ€æœ¯æ”¯æŒå›¢é˜Ÿ
- åœ¨æˆ‘ä»¬çš„ GitHub ä»“åº“æäº¤ [Issue](https://github.com/MiniMax-AI/MiniMax-M1/issues)

æˆ‘ä»¬ä¼šæŒç»­ä¼˜åŒ–æ¨¡å‹çš„éƒ¨ç½²ä½“éªŒï¼Œæ¬¢è¿æ‚¨çš„åé¦ˆï¼
