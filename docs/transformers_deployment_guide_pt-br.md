# üöÄ Guia de Deploy do Modelo MiniMax com Transformers

[Transformers‰∏≠ÊñáÁâàÈÉ®ÁΩ≤ÊåáÂçó](./transformers_deployment_guide_cn.md)

## üìñ Introdu√ß√£o

Este guia ir√° te ajudar a fazer o deploy do modelo MiniMax-M1 utilizando a biblioteca [Transformers](https://huggingface.co/docs/transformers/index). O Transformers √© uma biblioteca de deep learning amplamente utilizada, que oferece uma vasta cole√ß√£o de modelos pr√©-treinados e interfaces flex√≠veis para opera√ß√£o dos modelos.

## üõ†Ô∏è Configura√ß√£o do Ambiente

### Instalando o Transformers

```bash
pip install transformers torch accelerate
```

## üìã Exemplo de Uso B√°sico

O modelo pr√©-treinado pode ser utilizado da seguinte maneira:

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig

MODEL_PATH = "{MODEL_PATH}"
model = AutoModelForCausalLM.from_pretrained(MODEL_PATH, device_map="auto", trust_remote_code=True)
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)

messages = [
    {"role": "user", "content": [{"type": "text", "text": "What is your favourite condiment?"}]},
    {"role": "assistant", "content": [{"type": "text", "text": "Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!"}]},
    {"role": "user", "content": [{"type": "text", "text": "Do you have mayonnaise recipes?"}]}
]

text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)

model_inputs = tokenizer(text, return_tensors="pt").to(model.device)

generation_config = GenerationConfig(
    max_new_tokens=20,
    eos_token_id=tokenizer.eos_token_id,
    use_cache=True,
)

generated_ids = model.generate(**model_inputs, generation_config=generation_config)

generated_ids = [
    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
]

response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
print(response)
```

## ‚ö° Otimiza√ß√£o de Desempenho

### Acelerando com Flash Attention

O exemplo acima mostra uma infer√™ncia sem nenhum tipo de otimiza√ß√£o. No entanto, √© poss√≠vel acelerar significativamente o modelo utilizando [Flash Attention](../perf_train_gpu_one#flash-attention-2), que √© uma implementa√ß√£o mais r√°pida do mecanismo de aten√ß√£o usado no modelo.

Primeiro, certifique-se de instalar a vers√£o mais recente do Flash Attention 2:

```bash
pip install -U flash-attn --no-build-isolation
```

Al√©m disso, √© necess√°rio que seu hardware seja compat√≠vel com o Flash Attention 2. Consulte mais informa√ß√µes na documenta√ß√£o oficial do [reposit√≥rio do Flash Attention](https://github.com/Dao-AILab/flash-attention). Tamb√©m √© recomendado carregar seu modelo em meia precis√£o (por exemplo, `torch.float16`).

Para carregar e executar um modelo utilizando Flash Attention 2, utilize o exemplo abaixo:

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

MODEL_PATH = "{MODEL_PATH}"
model = AutoModelForCausalLM.from_pretrained(MODEL_PATH, trust_remote_code=True, torch_dtype=torch.float16, attn_implementation="flash_attention_2", device_map="auto")
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)

prompt = "My favourite condiment is"

model_inputs = tokenizer([prompt], return_tensors="pt").to("cuda")
generated_ids = model.generate(**model_inputs, max_new_tokens=100, do_sample=True)
response = tokenizer.batch_decode(generated_ids)[0]
print(response)
```

## üìÆ Suporte

Se voc√™ encontrar qualquer problema durante o deploy do modelo MiniMax-M1:

* Verifique nossa documenta√ß√£o oficial
* Entre em contato com nossa equipe de suporte t√©cnico pelos canais oficiais
* Abra uma Issue no nosso reposit√≥rio no GitHub

Estamos continuamente otimizando a experi√™ncia de deploy no Transformers e valorizamos muito seu feedback!
