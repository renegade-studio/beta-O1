# Guia de Uso de Function Call no MiniMax-M1

[FunctionCall‰∏≠Êñá‰ΩøÁî®ÊåáÂçó](./function_call_guide_cn.md)

## üìñ Introdu√ß√£o

O modelo MiniMax-M1 possui suporte para chamadas de fun√ß√µes (Function Call), permitindo que o modelo identifique quando fun√ß√µes externas precisam ser chamadas e gere os par√¢metros dessas chamadas em um formato estruturado. Este documento fornece instru√ß√µes detalhadas sobre como utilizar o recurso de chamadas de fun√ß√µes do MiniMax-M1.

## üöÄ In√≠cio R√°pido

### Usando vLLM para Function Calls (Recomendado)

Na implanta√ß√£o real, para suportar capacidades nativas de Function Calling (chamada de ferramentas) semelhantes √† API OpenAI, o modelo MiniMax-M1 integra um parser dedicado `tool_call_parser=minimax`, evitando an√°lise regex adicional da sa√≠da do modelo.

#### Configura√ß√£o do Ambiente e Recompila√ß√£o do vLLM

Como este recurso ainda n√£o foi oficialmente lan√ßado na vers√£o PyPI, √© necess√°ria compila√ß√£o a partir do c√≥digo fonte. O seguinte √© um processo de exemplo baseado na imagem oficial do Docker vLLM `vllm/vllm-openai:v0.8.3`:

```bash
IMAGE=vllm/vllm-openai:v0.8.3
DOCKER_RUN_CMD="--network=host --privileged --ipc=host --ulimit memlock=-1 --shm-size=32gb --rm --gpus all --ulimit stack=67108864"

# Executar docker
sudo docker run -it -v $MODEL_DIR:$MODEL_DIR \
                    -v $CODE_DIR:$CODE_DIR \
                    --name vllm_function_call \
                    $DOCKER_RUN_CMD \
                    --entrypoint /bin/bash \
                    $IMAGE
```

#### Compilando o C√≥digo Fonte do vLLM

Ap√≥s entrar no container, execute os seguintes comandos para obter o c√≥digo fonte e reinstalar:

```bash
cd $CODE_DIR
git clone https://github.com/vllm-project/vllm.git
cd vllm
pip install -e .
```

#### Iniciando o Servi√ßo API vLLM

```bash
export SAFETENSORS_FAST_GPU=1
export VLLM_USE_V1=0

python3 -m vllm.entrypoints.openai.api_server \
--model MiniMax-M1-80k \
--tensor-parallel-size 8 \
--trust-remote-code \
--quantization experts_int8  \
--enable-auto-tool-choice \
--tool-call-parser minimax \
--chat-template vllm/examples/tool_chat_template_minimax_m1.jinja \
--max_model_len 4096 \
--dtype bfloat16 \
--gpu-memory-utilization 0.85
```

**‚ö†Ô∏è Nota:**
- `--tool-call-parser minimax` √© um par√¢metro chave para habilitar o parser personalizado MiniMax-M1
- `--enable-auto-tool-choice` habilita a sele√ß√£o autom√°tica de ferramentas
- `--chat-template` arquivo de template precisa ser adaptado para o formato de chamada de ferramentas

#### Exemplo de Script de Teste de Function Call

O seguinte script Python implementa um exemplo de chamada de fun√ß√£o de consulta meteorol√≥gica baseado no SDK OpenAI:

```python
from openai import OpenAI
import json

client = OpenAI(base_url="http://localhost:8000/v1", api_key="dummy")

def get_weather(location: str, unit: str):
    return f"Getting the weather for {location} in {unit}..."

tool_functions = {"get_weather": get_weather}

tools = [{
    "type": "function",
    "function": {
        "name": "get_weather",
        "description": "Get the current weather in a given location",
        "parameters": {
            "type": "object",
            "properties": {
                "location": {"type": "string", "description": "City and state, e.g., 'San Francisco, CA'"},
                "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]}
            },
            "required": ["location", "unit"]
        }
    }
}]

response = client.chat.completions.create(
    model=client.models.list().data[0].id,
    messages=[{"role": "user", "content": "What's the weather like in San Francisco? use celsius."}],
    tools=tools,
    tool_choice="auto"
)

print(response)

tool_call = response.choices[0].message.tool_calls[0].function
print(f"Function called: {tool_call.name}")
print(f"Arguments: {tool_call.arguments}")
print(f"Result: {get_weather(**json.loads(tool_call.arguments))}")
```

**Exemplo de Sa√≠da:**
```
Function called: get_weather
Arguments: {"location": "San Francisco, CA", "unit": "celsius"}
Result: Getting the weather for San Francisco, CA in celsius...
```

### An√°lise Manual da Sa√≠da do Modelo

Se voc√™ n√£o puder usar o parser integrado do vLLM, ou precisar usar outros frameworks de infer√™ncia (como transformers, TGI, etc.), voc√™ pode usar o seguinte m√©todo para analisar manualmente a sa√≠da bruta do modelo. Este m√©todo requer que voc√™ analise o formato de tags XML da sa√≠da do modelo.

#### Exemplo Usando Transformers

O seguinte √© um exemplo completo usando a biblioteca transformers:

```python
from transformers import AutoTokenizer

def get_default_tools():
    return [
        {
          "name": "get_current_weather",
          "description": "Get the latest weather for a location",
          "parameters": {
              "type": "object", 
              "properties": {
                  "location": {
                      "type": "string", 
                      "description": "A certain city, such as Beijing, Shanghai"
                  }
              }, 
          }
          "required": ["location"],
          "type": "object"
        }
    ]

# Carregar modelo e tokenizador
tokenizer = AutoTokenizer.from_pretrained(model_id)
prompt = "What's the weather like in Shanghai today?"
messages = [
    {"role": "system", "content": [{"type": "text", "text": "You are a helpful assistant created by Minimax based on MiniMax-M1 model."}]},
    {"role": "user", "content": [{"type": "text", "text": prompt}]},
]

# Habilitar ferramentas de chamada de fun√ß√£o
tools = get_default_tools()

# Aplicar template de chat e adicionar defini√ß√µes de ferramentas
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True,
    tools=tools
)

# Enviar requisi√ß√£o (usando qualquer servi√ßo de infer√™ncia aqui)
import requests
payload = {
    "model": "MiniMaxAI/MiniMax-M1-40k",
    "prompt": text,
    "max_tokens": 4000
}
response = requests.post(
    "http://localhost:8000/v1/completions",
    headers={"Content-Type": "application/json"},
    json=payload,
    stream=False,
)

# Sa√≠da do modelo precisa de an√°lise manual
raw_output = response.json()["choices"][0]["text"]
print("Sa√≠da bruta:", raw_output)

# Use a fun√ß√£o de an√°lise abaixo para processar a sa√≠da
function_calls = parse_function_calls(raw_output)
```

## üõ†Ô∏è Defini√ß√£o de Function Call

### Estrutura da Fun√ß√£o

As fun√ß√µes precisam ser definidas no campo `tools` do corpo da requisi√ß√£o. Cada fun√ß√£o √© composta pelos seguintes elementos:

```json
{
  "tools": [
    {
      "name": "search_web",
      "description": "Fun√ß√£o de busca.",
      "parameters": {
        "properties": {
          "query_list": {
            "description": "Palavras-chave para busca, com contagem de elementos da lista de 1.",
            "items": { "type": "string" },
            "type": "array"
          },
          "query_tag": {
            "description": "Classifica√ß√£o da consulta",
            "items": { "type": "string" },
            "type": "array"
          }
        },
        "required": [ "query_list", "query_tag" ],
        "type": "object"
      }
    }
  ]
}
```

**Descri√ß√£o dos Campos:**

* `name`: Nome da fun√ß√£o
* `description`: Descri√ß√£o da fun√ß√£o
* `parameters`: Defini√ß√£o dos par√¢metros da fun√ß√£o

  * `properties`: Defini√ß√µes dos par√¢metros, onde a chave √© o nome do par√¢metro e o valor cont√©m a descri√ß√£o detalhada do par√¢metro
  * `required`: Lista de par√¢metros obrigat√≥rios
  * `type`: Tipo de par√¢metro (geralmente "object")

### Formato de Processamento Interno do Modelo

Quando processadas internamente pelo modelo, as defini√ß√µes de fun√ß√£o s√£o convertidas para um formato especial e concatenadas ao texto de entrada:

```
<begin_of_document><beginning_of_sentence>system ai_setting=MiniMax AI
MiniMax AIÊòØÁî±‰∏äÊµ∑Á®ÄÂÆáÁßëÊäÄÊúâÈôêÂÖ¨Âè∏ÔºàMiniMaxÔºâËá™‰∏ªÁ†îÂèëÁöÑAIÂä©ÁêÜ„ÄÇ<end_of_sentence>
<beginning_of_sentence>system tool_setting=tools
You are provided with these tools:
<tools>
{"name": "search_web", "description": "ÊêúÁ¥¢ÂáΩÊï∞„ÄÇ", "parameters": {"properties": {"query_list": {"description": "ËøõË°åÊêúÁ¥¢ÁöÑÂÖ≥ÈîÆËØçÔºåÂàóË°®ÂÖÉÁ¥†‰∏™Êï∞‰∏∫1„ÄÇ", "items": {"type": "string"}, "type": "array"}, "query_tag": {"description": "queryÁöÑÂàÜÁ±ª", "items": {"type": "string"}, "type": "array"}}, "required": ["query_list", "query_tag"], "type": "object"}}
</tools>
If you need to call tools, please respond with <tool_calls></tool_calls> XML tags, and provide tool-name and json-object of arguments, following the format below:
<tool_calls>
{"name": <tool-name>, "arguments": <args-json-object>}
...
</tool_calls><end_of_sentence>
<beginning_of_sentence>user name=Áî®Êà∑
OpenAI Âíå Gemini ÁöÑÊúÄËøë‰∏ÄÊ¨°ÂèëÂ∏É‰ºöÈÉΩÊòØ‰ªÄ‰πàÊó∂ÂÄô?<end_of_sentence>
<beginning_of_sentence>ai name=MiniMax AI
```

### Formato de Sa√≠da do Modelo

O modelo gera chamadas de fun√ß√£o no seguinte formato:

```xml
<think>
Ok, vou procurar a vers√£o mais recente do OpenAI e do Gemini.
</think>
<tool_calls>
{"name": "search_web", "arguments": {"query_tag": ["technology", "events"], "query_list": ["\"OpenAI\" \"latest\" \"release\""]}}
{"name": "search_web", "arguments": {"query_tag": ["technology", "events"], "query_list": ["\"Gemini\" \"latest\" \"release\""]}}
</tool_calls>
```

## üì• An√°lise Manual dos Resultados de Function Call

### Fazendo o Parse das Chamadas de Fun√ß√£o

Quando a an√°lise manual √© necess√°ria, voc√™ precisa analisar o formato de tags XML da sa√≠da do modelo:

```python
import re
import json
def parse_function_calls(content: str):
    """
    Analisar chamadas de fun√ß√£o da sa√≠da do modelo
    """
    function_calls = []
    
    # Corresponder conte√∫do dentro das tags <tool_calls>
    tool_calls_pattern = r"<tool_calls>(.*?)</tool_calls>"
    tool_calls_match = re.search(tool_calls_pattern, content, re.DOTALL)
    
    if not tool_calls_match:
        return function_calls
    
    tool_calls_content = tool_calls_match.group(1).strip()
    
    # Analisar cada chamada de fun√ß√£o (um objeto JSON por linha)
    for line in tool_calls_content.split('\n'):
        line = line.strip()
        if not line:
            continue
            
        try:
            # Analisar chamada de fun√ß√£o em formato JSON
            call_data = json.loads(line)
            function_name = call_data.get("name")
            arguments = call_data.get("arguments", {})
            
            function_calls.append({
                "name": function_name,
                "arguments": arguments
            })
            
            print(f"Chamada de fun√ß√£o: {function_name}, Argumentos: {arguments}")
            
        except json.JSONDecodeError as e:
            print(f"Falha na an√°lise de par√¢metros: {line}, Erro: {e}")
    
    return function_calls

# Exemplo: Manipular fun√ß√£o de consulta de clima
def execute_function_call(function_name: str, arguments: dict):
    """
    Executar chamada de fun√ß√£o e retornar resultado
    """
    if function_name == "get_current_weather":
        location = arguments.get("location", "Localiza√ß√£o desconhecida")
        # Construir resultado da execu√ß√£o da fun√ß√£o
        return {
            "role": "tool", 
            "content": [
              {
                "name": function_name,
                "type": "text",
                "text": json.dumps({
                    "location": location, 
                    "temperature": "25", 
                    "unit": "celsius", 
                    "weather": "Ensolarado"
                }, ensure_ascii=False)
              }
            ] 
          }
    elif function_name == "search_web":
        query_list = arguments.get("query_list", [])
        query_tag = arguments.get("query_tag", [])
        # Simular resultados de pesquisa
        return {
            "role": "tool",
            "content": [
              {
                "name": function_name,
                "type": "text",
                "text": f"Palavras-chave de busca: {query_list}, Categorias: {query_tag}\nResultados da busca: Informa√ß√µes relevantes encontradas"
              }
            ]
          }
    
    return None
```

### Retornando os Resultados da Execu√ß√£o de Fun√ß√£o para o Modelo

Ap√≥s analisar com sucesso as chamadas de fun√ß√£o, voc√™ deve adicionar os resultados da execu√ß√£o da fun√ß√£o ao hist√≥rico da conversa para que o modelo possa acessar e utilizar essas informa√ß√µes em intera√ß√µes subsequentes.

#### Resultado √önico

Se o modelo chamar a fun√ß√£o `search_web`, voc√™ pode se referir ao seguinte formato para adicionar resultados de execu√ß√£o, com o campo `name` sendo o nome espec√≠fico da fun√ß√£o.

```json
{
  "role": "tool", 
  "content": [
    {
      "name": "search_web",
      "type": "text",
      "text": "test_result"
    }
  ]
}
```

Formato de entrada correspondente do modelo:
```
<beginning_of_sentence>tool name=tools
tool name: search_web
tool result: test_result
<end_of_sentence>
```

#### V√°rios Resultados

Se o modelo chamar simultaneamente as fun√ß√µes `search_web` e `get_current_weather`, voc√™ pode se referir ao seguinte formato para adicionar resultados de execu√ß√£o, com `content` contendo v√°rios resultados.

```json
{
  "role": "tool", 
  "content": [
    {
      "name": "search_web",
      "type": "text",
      "text": "test_result1"
    },
    {
      "name": "get_current_weather",
      "type": "text",
      "text": "test_result2"
    }
  ]
}
```

Formato de entrada correspondente do modelo:
```
<beginning_of_sentence>tool name=tools
tool name: search_web
tool result: test_result1
tool name: get_current_weather
tool result: test_result2<end_of_sentence>
```

Embora recomendemos seguir os formatos acima, desde que a entrada retornada ao modelo seja f√°cil de entender, o conte√∫do espec√≠fico de `name` e `text` √© inteiramente de sua escolha.

## üìö Refer√™ncias

- [Reposit√≥rio do Modelo MiniMax-M1](https://github.com/MiniMaxAI/MiniMax-M1)
- [P√°gina Principal do Projeto vLLM](https://github.com/vllm-project/vllm)
- [PR de Function Calling do vLLM](https://github.com/vllm-project/vllm/pull/20297)
- [SDK Python OpenAI](https://github.com/openai/openai-python)